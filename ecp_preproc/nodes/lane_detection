#!/usr/bin/env python
# -*- coding: utf-8 -*-

################################################################################
# Copyright 2018 ROBOTIS CO., LTD.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
################################################################################

# Authors: Leon Jung, Gilbert, Ashe Kim, Special Thanks : Roger Sacchelli

import rospy
import itertools
import numpy as np
import copy
import cv2
from cv_bridge import CvBridge
from std_msgs.msg import UInt8, Float64, Bool
from sensor_msgs.msg import Image, CompressedImage
from dynamic_reconfigure.server import Server
from ecp_preproc.cfg import DetectLaneParamsConfig
from ecp_preproc.msg import DetectLaneInfo


class DetectLane():
    def __init__(self):

        self.image_width = 320
        self.image_height = 180

        self._val_trans_width, self._val_trans_height = self.image_width, self.image_height

        self._center_y = self.image_height//2
        self._center_x = self.image_width//2

        self.is_calibration_mode = rospy.get_param("~is_detection_calibration_mode", False)
        if self.is_calibration_mode == True:
            srv_detect_lane = Server(DetectLaneParamsConfig, self.cbGetDetectLaneParam)

        # subscribes compressed image
        self.sub_image_original = rospy.Subscriber('/detect/image_input/compressed', CompressedImage, self.cbFindLane, queue_size = 1)
        self.sub_detect_lane_side = rospy.Subscriber('/detect/lane_side', UInt8, self.cbGetLaneSide, queue_size = 1)
        self.sub_check_parking_ready = rospy.Subscriber('/check/parking_ready', UInt8, self.cbGetParkingReady, queue_size = 1)
        
        self.pub_img_binary = rospy.Publisher('/detect/image/binary/compressed', CompressedImage, queue_size=1)
        self.pub_dot_lane = rospy.Publisher('/detect/image/dot/compressed', CompressedImage, queue_size=1)
        self.pub_end_line = rospy.Publisher('/detect/image/end_line/compressed', CompressedImage, queue_size=1)
        
        self.pub_parking_state = rospy.Publisher('/parking/state', UInt8, queue_size = 1)
        self.pub_lane = rospy.Publisher('/detect/lane', DetectLaneInfo, queue_size = 1)
 
        self.cvBridge = CvBridge()

        self.detect_lane_side = 0 #0: both, 1: left, 2: right
        self.check_parking_ready = 0 #0: None. 1: Find Dot, 2: Find End-Line
        self.cv_image = None

        loop_rate = rospy.Rate(10) # 10hz
        while not rospy.is_shutdown():
            if self.check_parking_ready == 1:
                self.fnCheckParkingReady()
            elif self.check_parking_ready == 2:
                self.fn_find_stop()
            loop_rate.sleep()

    def cbGetParkingReady(self, msg):
        self.check_parking_ready = msg.data
        # rospy.loginfo("Check Parking Ready: %d", self.check_parking_ready)

    def cbGetLaneSide(self, msg):
        self.detect_lane_side = msg.data
        rospy.loginfo("Detect Lane Side: %d", self.detect_lane_side)

    def cbGetDetectLaneParam(self, config, level):
        rospy.loginfo("[Detect Lane] Detect Lane Calibration Parameter reconfigured to")
        rospy.loginfo("hue_white_l : %d", config.hue_white_l)
        return config
    
    def fn_lane_threshold(self, image, val_threshold=200):
        img_1ch = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        val_hist_percent = 1.0 # 3.0
        val_hist_size = 256

        if val_hist_percent == 0.0:
            val_stretch_low, val_stretch_high, _, _ = cv2.minMaxLoc(img_1ch)
        else:
            hist_ori = cv2.calcHist([img_1ch], [0], None, [val_hist_size], [0, val_hist_size])
            accumulator = np.cumsum(hist_ori)
            num_of_pixel = accumulator[val_hist_size - 1]
            num_of_clip = num_of_pixel * (val_hist_percent / 100.)

            for val_stretch_low in range(val_hist_size):
                if accumulator[val_stretch_low] >= accumulator[0] + num_of_clip:
                    break

            for val_stretch_high in range(val_hist_size-1, -1, -1):
                if accumulator[val_stretch_high] <= (num_of_pixel - num_of_clip):
                    break

        try:
            input_range = val_stretch_high - val_stretch_low
            alpha = float(val_hist_size - 1) / input_range
            beta = -val_stretch_low * alpha

            img_stretch = cv2.convertScaleAbs(img_1ch, -1, alpha, beta)
            _, img_threshold_low = cv2.threshold(img_1ch, val_stretch_low, 255, cv2.THRESH_BINARY)
            _, img_threshold_high = cv2.threshold(img_1ch, val_stretch_high, 255, cv2.THRESH_BINARY)
            img_stretch = cv2.bitwise_or(img_stretch, img_threshold_high, mask=img_threshold_low)
        except:
            img_stretch = np.zeros_like(img_1ch, dtype=np.uint8)

        _, img_threshold = cv2.threshold(img_stretch, val_threshold, 255, cv2.THRESH_BINARY)
        return img_threshold

    def fn_lane_detect(self, img_trans, img_threshold):
        self.fn_init_parameter()
        _, [flag_left_lane, flag_right_lane], img_debug = self.fn_lane_process(img_trans, img_threshold)

        return flag_left_lane, flag_right_lane, img_debug

    def fn_init_parameter(self): # 외부 호출
        self.cx_left_pre = int(self._val_trans_width / 8)
        self.cx_right_pre = int(self._val_trans_width * 7 / 8)
        self.shift_pre = int((self.cx_right_pre - self.cx_left_pre) / 2)
        self.gap_pre = self.cx_right_pre - self.cx_left_pre
        self.error_pre = 0.0

    def _fn_limit_parameter(self):
        if self.shift_pre < self._val_trans_width*11/32:
            self.shift_pre = int(self._val_trans_width*11/32)
        elif self.shift_pre > self._val_trans_width*13/32:
            self.shift_pre = int(self._val_trans_width*13/32)

        if self.cx_left_pre < 0:
            self.cx_left_pre = 0
        elif self.cx_left_pre > (self._val_trans_width * 1 / 2):
            self.cx_left_pre = (self._val_trans_width * 1 / 2)

        if self.cx_right_pre < (self._val_trans_width * 1 / 2):
            self.cx_right_pre = (self._val_trans_width * 1 / 2)
        elif self.cx_right_pre > self._val_trans_width:
            self.cx_right_pre = self._val_trans_width

        if self.gap_pre < self._val_trans_width*11/16:
            self.gap_pre = int(self._val_trans_width*11/16)
        elif self.gap_pre > self._val_trans_width*13/16:
            self.gap_pre = int(self._val_trans_width*13/16)

        if (self.cx_right_pre - self.cx_left_pre) < (self._val_trans_width / 4) :
            gap_now = (self._val_trans_width / 4) - (self.cx_right_pre - self.cx_left_pre)
            self.cx_left_pre -= int(gap_now / 2)
            self.cx_right_pre += int(gap_now / 2)

    def _fn_init_list_pts(self):
        self.list_right_pts_x = []
        self.list_right_pts_y = []
        self.list_left_pts_x = []
        self.list_left_pts_y = []

    def _fn_find_lane(self, img_trans, img_threshold):
        #TODO : 수직 방향으로 구간 분할하기 위한 값들
        val_threshold_h, val_threshold_w = img_threshold.shape[:2]
        val_section_h = 4
        num_of_section = int(val_threshold_h / val_section_h)

        #TODO : 리스트 초기값 생성
        self._fn_init_list_pts()
        list_cx_left = [self.cx_left_pre] * 5
        list_cx_right = [self.cx_right_pre] * 5
        list_gap = [self.gap_pre] * 5

        #TODO : 수직 방향으로 구간 분할하고 각 구간마다 컨투어이용해서 차선영역 검출 및 분리 (from 5)
        for n_section in range(5, num_of_section):
            section_top = val_threshold_h - (n_section + 1) * val_section_h
            img_section = img_threshold[section_top:section_top + val_section_h, :]
            list_section = []

            cy = int(section_top + val_section_h / 2)
            if np.count_nonzero(img_section) > 10:
                
                list_contour, _ = cv2.findContours(img_section, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
                for i in range(len(list_contour)):
                    contour = list_contour[i]
                    moment = cv2.moments(contour)
                    if moment['m00'] > 10:
                        xmid = int(moment['m10'] / moment['m00'])
                        list_section.append(xmid)
                        cv2.circle(img_trans, (xmid, cy), 4, (120, 0, 0), 1)

            if len(self.list_left_pts_x) >= 5:
                val_left_min_limit = 20
            elif len(self.list_right_pts_x) >= 5:
                val_left_min_limit = 40
            else:
                val_left_min_limit = 110

            if len(self.list_right_pts_x) >= 5:
                val_right_min_limit = 20
            elif len(self.list_left_pts_x) >= 5:
                val_right_min_limit = 40
            else:
                val_right_min_limit = 110

            if len(list_section) >= 2:
                list_left_gap = []
                list_right_gap = []
                for xmid in list_section:
                    list_left_gap.append(abs(list_cx_left[-1] - xmid))
                    list_right_gap.append(abs(list_cx_right[-1] - xmid))

                list_left_right_gap = list_left_gap + list_right_gap
                idx_left_right = list_left_right_gap.index(min(list_left_right_gap))

                if idx_left_right < len(list_section):
                    val_left_min = min(list_left_right_gap)
                    idx_left = idx_left_right
                    list_right_gap[idx_left] = 10000
                    idx_right = list_right_gap.index(min(list_right_gap))
                    val_right_min = min(list_right_gap)

                else:
                    val_right_min = min(list_left_right_gap)
                    idx_right = idx_left_right - len(list_section)
                    list_left_gap[idx_right] = 10000
                    idx_left = list_left_gap.index(min(list_left_gap))
                    val_left_min = min(list_left_gap)

                if val_left_min < val_left_min_limit and val_right_min < val_right_min_limit:
                    cx_left = list_section[idx_left]
                    cx_right = list_section[idx_right]

                    list_cx_left.append(cx_left)
                    list_cx_right.append(cx_right)
                    list_gap.append(cx_right - cx_left)

                    self.list_left_pts_x.append(cx_left)
                    self.list_left_pts_y.append(cy)
                    self.list_right_pts_x.append(cx_right)
                    self.list_right_pts_y.append(cy)
                    cv2.circle(img_trans, (cx_left, cy), 4, (0, 0, 255), 1)
                    cv2.circle(img_trans, (cx_right, cy), 4, (0, 255, 0), 1)

                elif val_left_min < val_left_min_limit:
                    cx = list_section[idx_left]
                    list_cx_left.append(cx)
                    list_cx_right.append(cx + list_gap[-1])
                    list_gap.append(list_gap[-1])

                    self.list_left_pts_x.append(cx)
                    self.list_left_pts_y.append(cy)
                    cv2.circle(img_trans, (cx, cy), 4, (0, 0, 255), 1)

                elif val_right_min < val_left_min_limit:
                    cx = list_section[idx_right]
                    list_cx_right.append(cx)
                    list_cx_left.append(cx - list_gap[-1])
                    list_gap.append(list_gap[-1])

                    self.list_right_pts_x.append(cx)
                    self.list_right_pts_y.append(cy)
                    cv2.circle(img_trans, (cx, cy), 4, (0, 255, 0), 1)

                else:
                    list_cx_left.append(list_cx_left[-1] + int((list_cx_left[-1] - list_cx_left[-5]) / 4))
                    list_cx_right.append(list_cx_right[-1] + int((list_cx_right[-1] - list_cx_right[-5]) / 4))
                    list_gap.append(list_cx_right[-1] - list_cx_left[-1])

            elif len(list_section) == 1:
                [cx] = list_section
                left_gap = abs(list_cx_left[-1] - cx)
                right_gap = abs(list_cx_right[-1] - cx)

                if left_gap < right_gap:
                    if left_gap < val_left_min_limit:
                        list_cx_left.append(cx)
                        list_cx_right.append(cx + list_gap[-1])
                        list_gap.append(list_gap[-1])

                        self.list_left_pts_x.append(cx)
                        self.list_left_pts_y.append(cy)
                        cv2.circle(img_trans, (cx, cy), 4, (0, 0, 255), 1)

                else:
                    if right_gap < val_right_min_limit:
                        list_cx_right.append(cx)
                        list_cx_left.append(cx - list_gap[-1])
                        list_gap.append(list_gap[-1])

                        self.list_right_pts_x.append(cx)
                        self.list_right_pts_y.append(cy)
                        cv2.circle(img_trans, (cx, cy), 4, (0, 255, 0), 1)

            else:
                list_cx_left.append(list_cx_left[-1] + int((list_cx_left[-1] - list_cx_left[-5]) / 4))
                list_cx_right.append(list_cx_right[-1] + int((list_cx_right[-1] - list_cx_right[-5]) / 4))
                list_gap.append(list_cx_right[-1] - list_cx_left[-1])

    def fn_lane_process(self, img_trans, img_threshold, right_lane=True, left_lane=True): # 외부 호출
        num_pit_lane = 15

        #TODO : morphology 연산으로 노이즈 제거
        img_threshold = cv2.morphologyEx(img_threshold, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))
        img_threshold = cv2.morphologyEx(img_threshold, cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8))
        val_threshold_h, val_threshold_w = img_threshold.shape[:2]

        #TODO : 양쪽 라인을 찾는 함수
        self._fn_find_lane(img_trans, img_threshold)

        #TODO : 라인 찾기 플래그
        if len(self.list_left_pts_x) > num_pit_lane:
            flag_find_left_lane = True
        else:
            flag_find_left_lane = False

        if len(self.list_right_pts_x) > num_pit_lane:
            flag_find_right_lane = True
        else:
            flag_find_right_lane = False

        #TODO : 차선 없애기
        if not right_lane:
            self.list_right_pts_x = []
            self.list_right_pts_y = []
        if not left_lane:
            self.list_left_pts_x = []
            self.list_left_pts_y = []

        arr_x = None
        arr_y = np.linspace(0, val_threshold_h - 1, val_threshold_h)
        lane_pos = 0

        #TODO : 양쪽 차선 모두 검출됬을 때 가운데 선 추출
        if len(self.list_left_pts_x) > num_pit_lane and len(self.list_right_pts_x) > num_pit_lane:
            try:
                img_trans, arr_left_x = self.fnDrawLeftLane(img_trans, arr_y)
                img_trans, arr_right_x = self.fnDrawRightLane(img_trans, arr_y)

                if self.detect_lane_side == 0:
                    arr_x = np.mean([arr_left_x, arr_right_x], axis=0)
                    img_trans = self.fnDrawMidLane(img_trans, arr_x, arr_y)
                    
                    self.shift_pre = int(abs(arr_right_x[self._center_y] - arr_x[self._center_y]))
                    self.cx_left_pre = int(arr_left_x[self._center_y])
                    self.cx_right_pre = int(arr_right_x[self._center_y])
                    self.gap_pre = self.shift_pre * 2
                    lane_pos = 0

                elif self.detect_lane_side == 1:
                    arr_x = np.add(arr_left_x, self.shift_pre *  2*2 / 3)
                    img_trans = self.fnDrawMidLane(img_trans, arr_x, arr_y)
                    self.cx_left_pre = int(arr_left_x[self._center_y])
                    self.cx_right_pre = int(arr_left_x[self._center_y] + self.shift_pre * 2)
                    self.gap_pre = self.shift_pre * 2

                    lane_pos = 1

                elif self.detect_lane_side == 2:
                    arr_x = np.subtract(arr_right_x, self.shift_pre * 2 *2 / 3)
                    img_trans = self.fnDrawMidLane(img_trans, arr_x, arr_y)
                    self.cx_right_pre = int(arr_right_x[self._center_y])
                    self.cx_left_pre = int(arr_right_x[self._center_y] - self.shift_pre * 2)
                    self.gap_pre = self.shift_pre * 2

                    lane_pos = 2

            except Exception as e:
                rospy.logerr("Fail.both : " + e)

        #TODO : 왼쪽 차선만 검출됬을 때 shift
        elif len(self.list_left_pts_x) > num_pit_lane and len(self.list_right_pts_x) <= num_pit_lane:
            try:
                img_trans, arr_left_x = self.fnDrawLeftLane(img_trans, arr_y)
                arr_x = np.add(arr_left_x, self.shift_pre * 2 * 2/ 3)
                img_trans = self.fnDrawMidLane(img_trans, arr_x, arr_y)
                self.cx_left_pre = int(arr_left_x[self._center_y])
                self.cx_right_pre = int(arr_left_x[self._center_y] + self.shift_pre * 2)
                self.gap_pre = self.shift_pre * 2

                lane_pos = 1
                
            except Exception as e:
                rospy.logerr("Fail.left : " + e)

        #TODO : 오른쪽 차선만 검출됬을 때 shift
        elif len(self.list_left_pts_x) <= num_pit_lane and len(self.list_right_pts_x) > num_pit_lane:
            try:
                img_trans, arr_right_x = self.fnDrawRightLane(img_trans, arr_y)
                arr_x = np.subtract(arr_right_x, self.shift_pre * 2 * 2/ 3)
                img_trans = self.fnDrawMidLane(img_trans, arr_x, arr_y)
                self.cx_right_pre = int(arr_right_x[self._center_y])
                self.cx_left_pre = int(arr_right_x[self._center_y] - self.shift_pre * 2)
                self.gap_pre = self.shift_pre * 2

                lane_pos = 2

            except Exception as e:
                rospy.logerr("Fail.right : " + e)

        #TODO : 중앙선 만들었을 경우
        if arr_x is not None:
            self._fn_limit_parameter()

            err = arr_x[self._center_y] - self._center_x
            self.error_pre = err
            rospy.loginfo("Error : " + str(err))

            cv2.circle(img_trans, (self._center_x, self._center_y*3//4), 8, (255, 255, 0), 2)
            cv2.circle(img_trans, (int(arr_x[self._center_y*3//4]), self._center_y*3//4), 8, (255, 0, 0), 2)

            msg_detect_lane = DetectLaneInfo()
            msg_detect_lane.desired_center = arr_x[self._center_y*3//4]
            msg_detect_lane.lane_pos = lane_pos
            self.pub_lane.publish(msg_detect_lane)

        #TODO : lane not detect 발생
        else:
            rospy.logwarn("lane not found")
            err = self.error_pre

        #TODO : 리턴 값
        img_threshold_3ch = cv2.merge([img_threshold, img_threshold, img_threshold])
        img_debug = np.hstack((img_threshold_3ch, img_trans))

        return err, [flag_find_left_lane, flag_find_right_lane], img_debug

    def fnDrawLeftLane(self,img_trans, arr_y):
        list_left_lane_fit = np.polyfit(self.list_left_pts_y, self.list_left_pts_x, 2)
        arr_left_x = list_left_lane_fit[0] * arr_y ** 2 + list_left_lane_fit[1] * arr_y + list_left_lane_fit[2]
        arr_pts = np.array([np.transpose(np.vstack([arr_left_x, arr_y]))])
        cv2.polylines(img_trans, np.int_([arr_pts]), isClosed=False, color=(255, 0, 255), thickness=3)
        return img_trans, arr_left_x
    
    def fnDrawRightLane(self,img_trans, arr_y):
        list_right_lane_fit = np.polyfit(self.list_right_pts_y, self.list_right_pts_x, 2)
        arr_right_x = list_right_lane_fit[0] * arr_y ** 2 + list_right_lane_fit[1] * arr_y + list_right_lane_fit[2]
        arr_pts = np.array([np.transpose(np.vstack([arr_right_x, arr_y]))])
        cv2.polylines(img_trans, np.int_([arr_pts]), isClosed=False, color=(255, 255, 0), thickness=3)
        return img_trans, arr_right_x
    
    def fnDrawMidLane(self, img_trans, arr_x, arr_y):
        arr_pts = np.array([np.transpose(np.vstack([arr_x, arr_y]))])
        cv2.polylines(img_trans, np.int_([arr_pts]), isClosed=False, color=(255, 0, 0), thickness=3)
        return img_trans

    def cbFindLane(self, image_msg):
        #converting compressed image to opencv image
        np_arr = np.frombuffer(image_msg.data, np.uint8)
        self.cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
        cv_image = self.cv_image.copy()
        img_threshold = self.fn_lane_threshold(cv_image)
        detect_left_lane, detect_right_lane, img_debug = self.fn_lane_detect(cv_image,img_threshold)
    
        self.pub_img_binary.publish(self.cvBridge.cv2_to_compressed_imgmsg(img_debug, "jpg"))

    def fnCheckParkingReady(self):
        if self.cv_image is not None:
            flag_left_dot, img_left_dot = self.fn_find_dot(self.cv_image, direction='left')
            flag_right_dot, img_right_dot = self.fn_find_dot(self.cv_image, direction='right')
            img_dot = np.hstack((img_left_dot, img_right_dot))

            msg_dot_pos = UInt8()
            msg_dot_pos.data = 0
            if flag_left_dot or flag_right_dot:
                msg_dot_pos.data = 1
            self.pub_parking_state.publish(msg_dot_pos)
            self.pub_dot_lane.publish(self.cvBridge.cv2_to_compressed_imgmsg(img_dot, "jpg"))

    def fn_find_dot(self, img_trans, direction='left'):
        dotlane_check = False
        parking_area_check = False

        # ROI 영역에 맞게 자른 이미지
        pers_height, pers_width = img_trans.shape[:2]  # shape is w384 x h240
        if direction == 'left':
            img_gray = cv2.cvtColor(img_trans[:, :int(pers_width *5/ 8)].copy(), cv2.COLOR_RGB2GRAY)
        else:
            img_gray = cv2.cvtColor(img_trans[:, int(pers_width *3/ 8):].copy(), cv2.COLOR_RGB2GRAY)
        _, img_dot = cv2.threshold(img_gray, 200, 255, 0)
        img_debug = cv2.merge((img_dot, img_dot, img_dot))
        img_dot = cv2.morphologyEx(img_dot, cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8))
        img_dot = cv2.morphologyEx(img_dot, cv2.MORPH_OPEN, np.ones((5, 5), np.uint8))

        # Dot Lane 변환 및 좌표정보
        list_contour_dotlane, _ = cv2.findContours(img_dot, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        # Dot Lane 이미지 생성
        list_all_dotlane_pos = []

        for contour_dotlane in list_contour_dotlane:
            cv2.drawContours(img_debug, [contour_dotlane], 0, (0, 0, 255), 2)
            x_dot, y_dot, w_dot, h_dot = cv2.boundingRect(contour_dotlane)

            cv2.putText(img_debug, 'w: {}, h: {}'.format(w_dot, h_dot), (contour_dotlane[0][0][0]+10, contour_dotlane[0][0][1]+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255))
            if (10 < w_dot < 30) and (20 < h_dot < 35):
                cv2.drawContours(img_debug, [contour_dotlane], 0, (0, 255, 0), 2)
                moment = cv2.moments(contour_dotlane)
                area = moment['m00']
                if area > 10:
                    cx = int(moment['m10'] / moment['m00'])
                    cy = int(moment['m01'] / moment['m00'])
                    #print(cx, cy, w_dot, h_dot, area)
                    list_all_dotlane_pos.append([cx, cy, w_dot, h_dot, area])

        # num_dot = 3
        num_dot = 2
        if len(list_all_dotlane_pos) >= num_dot:
            list_combination_dotlane_pos = list(itertools.combinations(list_all_dotlane_pos, num_dot))

            for list_dotlane_pos in list_combination_dotlane_pos:
                if True:
                    dotlane_check = True
                    break

        return dotlane_check, img_debug

    def fn_find_stop(self):
        img_trans = self.cv_image.copy()
        # ROI 영역에 맞게 자른 이미지
        pers_height, pers_width = img_trans.shape[:2]  # shape is w384 x h240
        img_gray = cv2.cvtColor(img_trans[int(pers_height * 1/ 4):int(pers_height * 3/ 4), :].copy(), cv2.COLOR_RGB2GRAY)
        # img_gray = cv2.cvtColor(img_trans.copy(), cv2.COLOR_RGB2GRAY)
        _, img_stop = cv2.threshold(img_gray, 200, 255, 0)
        img_debug = cv2.merge((img_stop, img_stop, img_stop))

        # Stop 관련
        contours_stop, _ = cv2.findContours(img_stop, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        msg_stop_check = UInt8()
        msg_stop_check.data = 1
        # Stop 이미지 생성
        for cnt in contours_stop:  # 점선 인식
            cv2.drawContours(img_debug, [cnt], 0, (0, 0, 255), 2)
            x_stop, y_stop, w_stop, h_stop = cv2.boundingRect(cnt)
            cv2.putText(img_debug, 'w: {}'.format(w_stop), (x_stop+10, y_stop+40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0))
            if 280 < w_stop:  # 주차공간 판단
                cv2.drawContours(img_debug, [cnt], 0, (0, 255, 0), 2)
                #print(w_stop)
                msg_stop_check.data = 2

        self.pub_parking_state.publish(msg_stop_check)
        self.pub_end_line.publish(self.cvBridge.cv2_to_compressed_imgmsg(img_debug, "jpg"))

    def main(self):
        rospy.spin()

if __name__ == '__main__':
    rospy.init_node('detect_lane')
    node = DetectLane()
    node.main()